{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Script Scrape\n",
    "\n",
    "Search Script Scrape was created by Dan Nguyen, to learn his students at Stanford Journalism College how to scrape by writing 101 scrapers. Below you'll find the scrapers as written by me, Winny de Jong. Some scrapers are written multiple times using different libraries/techniques. Just because. \n",
    "\n",
    "- [The original Github Repository](https://github.com/stanfordjournalism/search-script-scrape)\n",
    "- Article by Dan Nguyen [with background info](http://blog.danwin.com/examples-of-web-scraping-in-python-3-x-for-data-journalists/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scraper 001, v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 001 using requests and lxml  \n",
    "Goal: get the number of datasets currently listed on [data.gov](http://www.data.gov/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:30:54.179483Z",
     "start_time": "2018-04-08T01:30:54.063843Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237,527 datasets\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('http://www.data.gov/')\n",
    "\n",
    "# save html as text in variable doc\n",
    "doc = html.fromstring(site.text)\n",
    "\n",
    "# select the first [0] link found by selecting css; save it as ds1\n",
    "datasets = doc.cssselect('small a')[0]\n",
    "\n",
    "# print result\n",
    "print(datasets.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper 001, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper 001 - using beautifulsoup and requests  \n",
    "Goal:  get the number of datasets currently listed on [data.gov](http://www.data.gov/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:33:00.610707Z",
     "start_time": "2018-04-08T01:33:00.341153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'237,527'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('http://www.data.gov/')\n",
    "\n",
    "# make bs object from website\n",
    "soup = BeautifulSoup(site.content, 'html.parser')\n",
    "\n",
    "# save content div with class new-results in datasets\n",
    "datasets = soup.find('small')\n",
    "\n",
    "# save datasets as text in variable dt\n",
    "dt = datasets.text\n",
    "\n",
    "# slice dt to only get the number\n",
    "dt[12:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper 002, v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper 002 - using requests and lxml and css select  \n",
    "Goal: get the name of the [most recently added dataset on data.gov](https://catalog.data.gov/dataset?q=&sort=metadata_created+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:35:41.382022Z",
     "start_time": "2018-04-08T01:35:41.228862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "        Loudoun LandUse Existing Parcels\n",
      "        \n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('https://catalog.data.gov/dataset?q=&sort=metadata_created+desc')\n",
    "\n",
    "# save html as text in variable doc\n",
    "doc = html.fromstring(site.text)\n",
    "\n",
    "# select the first [0] link found by selecting css; save it as ds1\n",
    "ds1 = doc.cssselect('h3')[0].text_content()\n",
    "\n",
    "# print result\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper 002, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper 002 - using requests and lxml and css select  \n",
    "Goal: get the name of the [most recently added dataset on data.gov](https://catalog.data.gov/dataset?q=&sort=metadata_created+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:50:36.980820Z",
     "start_time": "2018-04-08T01:50:36.677906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loudoun LandUse Existing Parcels'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# create a variable with the url\n",
    "url = 'https://catalog.data.gov/dataset?q=&sort=metadata_created+desc'\n",
    "\n",
    "# Use requests to get the contents\n",
    "r = requests.get(url)\n",
    "\n",
    "# get the text of the contents\n",
    "html_content = r.text\n",
    "\n",
    "# convert the html content into a beautiful soup object\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# get text of first h3 heading\n",
    "soup.h3.a.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO Scraper 003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TO DO Scraper 004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:37:13.075795Z",
     "start_time": "2018-04-08T01:37:13.070514Z"
    },
    "hidden": true
   },
   "source": [
    "Scraper 004 - using ....  \n",
    "Goal: get the number of librarian-related job positions that the federal government is currently hiring for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:44:00.058125Z",
     "start_time": "2018-04-08T01:43:55.659949Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# collect basic info in vars\n",
    "code = 1410\n",
    "url = \"https://www.usajobs.gov/Search/?k=\"\n",
    "\n",
    "\"\"\"\n",
    "<h4 id=\"page-info\" class=\"usajobs-search-controls__results-count\">\n",
    "            Viewing 1 â€“ 9 of 9 jobs\n",
    "        </h4>\n",
    "\"\"\"\n",
    "\n",
    "# do request\n",
    "r = requests.get(url+str(code))\n",
    "# get text, save to html\n",
    "html = r.text\n",
    "# create soup using html\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "# find h4 with class usajobs-search-controls__results-count\n",
    "# print number of librarian-related job positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TO DO Scraper 005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
