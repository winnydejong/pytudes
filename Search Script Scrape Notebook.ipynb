{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Script Scrape\n",
    "\n",
    "Search Script Scrape was created by Dan Nguyen, to learn his students at Stanford Journalism College how to scrape by writing 101 scrapers. Below you'll find the scrapers as written by me, Winny de Jong. Some scrapers are written multiple times using different libraries/techniques. Just because. \n",
    "\n",
    "- [The original Github Repository](https://github.com/stanfordjournalism/search-script-scrape)\n",
    "- Article by Dan Nguyen [with background info](http://blog.danwin.com/examples-of-web-scraping-in-python-3-x-for-data-journalists/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scraper 001, v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 001 using requests and lxml  \n",
    "Goal: get the number of datasets currently listed on [data.gov](http://www.data.gov/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:30:54.179483Z",
     "start_time": "2018-04-08T01:30:54.063843Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237,527 datasets\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('http://www.data.gov/')\n",
    "\n",
    "# save html as text in variable doc\n",
    "doc = html.fromstring(site.text)\n",
    "\n",
    "# select the first [0] link found by selecting css; save it as ds1\n",
    "datasets = doc.cssselect('small a')[0]\n",
    "\n",
    "# print result\n",
    "print(datasets.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scraper 001, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 001 - using beautifulsoup and requests  \n",
    "Goal:  get the number of datasets currently listed on [data.gov](http://www.data.gov/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:33:00.610707Z",
     "start_time": "2018-04-08T01:33:00.341153Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'237,527'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('http://www.data.gov/')\n",
    "\n",
    "# make bs object from website\n",
    "soup = BeautifulSoup(site.content, 'html.parser')\n",
    "\n",
    "# save content div with class new-results in datasets\n",
    "datasets = soup.find('small')\n",
    "\n",
    "# save datasets as text in variable dt\n",
    "dt = datasets.text\n",
    "\n",
    "# slice dt to only get the number\n",
    "dt[12:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scraper 002, v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 002 - using requests and lxml and css select  \n",
    "Goal: get the name of the [most recently added dataset on data.gov](https://catalog.data.gov/dataset?q=&sort=metadata_created+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:35:41.382022Z",
     "start_time": "2018-04-08T01:35:41.228862Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "        Loudoun LandUse Existing Parcels\n",
      "        \n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('https://catalog.data.gov/dataset?q=&sort=metadata_created+desc')\n",
    "\n",
    "# save html as text in variable doc\n",
    "doc = html.fromstring(site.text)\n",
    "\n",
    "# select the first [0] link found by selecting css; save it as ds1\n",
    "ds1 = doc.cssselect('h3')[0].text_content()\n",
    "\n",
    "# print result\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scraper 002, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 002 - using requests and lxml and css select  \n",
    "Goal: get the name of the [most recently added dataset on data.gov](https://catalog.data.gov/dataset?q=&sort=metadata_created+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:50:36.980820Z",
     "start_time": "2018-04-08T01:50:36.677906Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loudoun LandUse Existing Parcels'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# create a variable with the url\n",
    "url = 'https://catalog.data.gov/dataset?q=&sort=metadata_created+desc'\n",
    "\n",
    "# Use requests to get the contents\n",
    "r = requests.get(url)\n",
    "\n",
    "# get the text of the contents\n",
    "html_content = r.text\n",
    "\n",
    "# convert the html content into a beautiful soup object\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# get text of first h3 heading\n",
    "soup.h3.a.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scraper 003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 003, using requests and navigating a json file  \n",
    "Goal: get the number of [people who visited a U.S. government website](https://analytics.usa.gov/data/live/ie.json) using Internet Explorer 6.0 in the last 90 days \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "site = \"https://analytics.usa.gov/data/live/ie.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T11:51:08.460630Z",
     "start_time": "2018-04-08T11:51:07.810674Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10511\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://analytics.usa.gov/data/live/ie.json\")\n",
    "# read json file with .json(), select info totals > ie_version > 6.0\n",
    "ie6 = (r.json()['totals']['ie_version']['6.0'])\n",
    "print(ie6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TO DO Scraper 004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:37:13.075795Z",
     "start_time": "2018-04-08T01:37:13.070514Z"
    },
    "hidden": true
   },
   "source": [
    "Scraper 004 - using using requests and navigating a json file  \n",
    "Goal: get the number of librarian-related job positions that the federal government is currently hiring for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T12:44:10.881485Z",
     "start_time": "2018-04-08T12:44:10.872634Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<h4 id=\"page-info\" class=\"usajobs-search-controls__results-count\">\\n            Viewing 1 – 9 of 9 jobs\\n        </h4>\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# collect basic info in vars\n",
    "code = 1410\n",
    "url = \"https://www.usajobs.gov/Search/?k=\"\n",
    "\n",
    "\"\"\"\n",
    "<h4 id=\"page-info\" class=\"usajobs-search-controls__results-count\">\n",
    "            Viewing 1 – 9 of 9 jobs\n",
    "        </h4>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## TO DO Scraper 005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Scraper 005, using ...  \n",
    "Goal: get the name of the company cited in the most recent consumer complaint involving student loans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T13:06:36.113211Z",
     "start_time": "2018-04-08T13:05:59.512981Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "match() missing 2 required positional arguments: 'pattern' and 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-7f0be47008e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# get companyName from card\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcompanyName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m '''\n",
      "\u001b[0;31mTypeError\u001b[0m: match() missing 2 required positional arguments: 'pattern' and 'string'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = \"https://www.consumerfinance.gov/data-research/consumer-complaints/search/?from=0&product=Student%20loan&product=Student%20loan%E2%80%A2Non-federal%20student%20loan&product=Student%20loan%E2%80%A2Federal%20student%20loan%20servicing&product=Student%20loan%E2%80%A2Private%20student%20loan&searchField=all&searchText=&size=25&sort=created_date_desc\"\n",
    "\n",
    "# save webpage to r\n",
    "r = requests.get(url)\n",
    "\n",
    "# get html\n",
    "html = r.text\n",
    "\n",
    "# convert the html content into a beautiful soup object\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# get company cards of first company div heading\n",
    "card = soup.find('div', attrs={'class': 'card-left'})\n",
    "\n",
    "# get companyName from card\n",
    "companyName = re.match()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "<div class=\"card-left layout-column\"><h3 class=\"to-detail\"><a href=\"detail/2866453\">2866453</a></h3><h4>Company name</h4><span class=\"body-copy\">Bank of CFPB, Washington, DC CFPB PIV</span><br><h4>Company response to consumer</h4><span class=\"body-copy\">Closed with explanation</span><br><h4>Timely response?</h4><span class=\"body-copy\">Yes</span></div>\n",
    "\n",
    "\n",
    "= requests.get(url)\n",
    "\n",
    "# get the text of the contents\n",
    "html_content = r.text\n",
    "\n",
    "# convert the html content into a beautiful soup object\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# get text of first h3 heading\n",
    "soup.h3.a.get_text()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Scraper ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scraper ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
